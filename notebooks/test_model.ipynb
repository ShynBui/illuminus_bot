{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b833b7b-b650-4007-99cd-d196087c78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(os.path.dirname(os.getcwd()), 'models', 'checkpoint-36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489bf4f1-a744-4ae8-bbd5-39a8b3d8fc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcd06b3cffc45fda2e0fc5cc0aa0d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tải model và tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7b81fc-6970-4ce4-be41-39d883ccf6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_files(train_json_path, test_json_path):\n",
    "    \"\"\"\n",
    "    Hàm nhận vào đường dẫn đến các tệp JSON chứa train và test, sau đó tạo DatasetDict\n",
    "    theo cấu trúc messages với role \"system\", \"user\", và \"assistant\".\n",
    "    \n",
    "    Args:\n",
    "        train_json_path (str): Đường dẫn đến tệp JSON chứa dữ liệu huấn luyện.\n",
    "        test_json_path (str): Đường dẫn đến tệp JSON chứa dữ liệu kiểm tra.\n",
    "        \n",
    "    Returns:\n",
    "        DatasetDict: Bao gồm tập train và test dưới dạng Hugging Face Dataset.\n",
    "    \"\"\"\n",
    "    # Bước 1: Đọc tệp JSON train và test\n",
    "    with open(train_json_path, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    with open(test_json_path, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    # Khởi tạo danh sách train_data và test_data để lưu cặp input-output\n",
    "    train_samples = []\n",
    "    test_samples = []\n",
    "\n",
    "    # Bước 2: Tiền xử lý và tách các đoạn hội thoại thành các cặp input-output cho tập train\n",
    "    for item in train_data:\n",
    "        previous_context = item[\"previous_context\"]\n",
    "        topic = item[\"topic\"]\n",
    "        language = item[\"language\"]\n",
    "        conversation = item[\"conversation\"]\n",
    "\n",
    "        for i in range(len(conversation) - 1):\n",
    "            if conversation[i][\"speaker\"] == \"David\" and conversation[i + 1][\"speaker\"] == \"Choi\":\n",
    "                # Input: Tạo đoạn hội thoại phù hợp với \"user\"\n",
    "                input_message = (f\"Previous context: {previous_context}\\n\"\n",
    "                                 f\"Topic: {topic}\\n\"\n",
    "                                 f\"Language: {language}\\n\"\n",
    "                                 f\"David's Emotion: {conversation[i]['emotion']}\\n\"\n",
    "                                 f\"Choi's Role: {conversation[i+1]['role']}\\n\"\n",
    "                                 f\"Choi's Emotion: {conversation[i+1]['emotion']}\\n\")\n",
    "\n",
    "                # Output: Cả hội thoại giữa David và Choi\n",
    "                output_message = (f\"David's Text: {conversation[i]['text']}\\n\"\n",
    "                                  f\"Choi's Text: {conversation[i+1]['text']}\")\n",
    "\n",
    "                # Tạo cấu trúc messages\n",
    "                conversation_messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"Your task is to generate a conversation between David and Choi. David will ask questions or talk based on his emotions and context, and Choi will respond appropriately according to his role and emotion.\"},\n",
    "                    {\"role\": \"user\", \"content\": input_message},\n",
    "                    {\"role\": \"assistant\", \"content\": output_message}\n",
    "                ]\n",
    "\n",
    "                # Thêm cặp input-output vào train_data\n",
    "                train_samples.append({\n",
    "                    \"messages\": conversation_messages\n",
    "                })\n",
    "\n",
    "    # Tương tự cho tập test\n",
    "    for item in test_data:\n",
    "        previous_context = item[\"previous_context\"]\n",
    "        topic = item[\"topic\"]\n",
    "        language = item[\"language\"]\n",
    "        conversation = item[\"conversation\"]\n",
    "\n",
    "        for i in range(len(conversation) - 1):\n",
    "            if conversation[i][\"speaker\"] == \"David\" and conversation[i + 1][\"speaker\"] == \"Choi\":\n",
    "                # Input: Tạo đoạn hội thoại phù hợp với \"user\"\n",
    "                input_message = (f\"Previous context: {previous_context}\\n\"\n",
    "                                 f\"Topic: {topic}\\n\"\n",
    "                                 f\"Language: {language}\\n\"\n",
    "                                 f\"David's Emotion: {conversation[i]['emotion']}\\n\"\n",
    "                                 f\"Choi's Role: {conversation[i+1]['role']}\\n\"\n",
    "                                 f\"Choi's Emotion: {conversation[i+1]['emotion']}\\n\")\n",
    "\n",
    "                # Output: Cả hội thoại giữa David và Choi\n",
    "                output_message = (f\"David's Text: {conversation[i]['text']}\\n\"\n",
    "                                  f\"Choi's Text: {conversation[i+1]['text']}\")\n",
    "\n",
    "                # Tạo cấu trúc messages\n",
    "                conversation_messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"Your task is to generate a conversation between David and Choi. David will ask questions or talk based on his emotions and context, and Choi will respond appropriately according to his role and emotion.\"},\n",
    "                    {\"role\": \"user\", \"content\": input_message},\n",
    "                    {\"role\": \"assistant\", \"content\": output_message}\n",
    "                ]\n",
    "\n",
    "                # Thêm cặp input-output vào test_data\n",
    "                test_samples.append({\n",
    "                    \"messages\": conversation_messages\n",
    "                })\n",
    "\n",
    "    # Bước 3: Tạo Dataset cho train và test bằng DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": Dataset.from_list(train_samples),\n",
    "        \"test\": Dataset.from_list(test_samples)\n",
    "    })\n",
    "\n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6bd960-919f-4df3-ba15-c68531bd86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (os.path.join(os.path.dirname(os.getcwd()), 'data', 'train_data.json'))\n",
    "test_path = (os.path.join(os.path.dirname(os.getcwd()), 'data', 'test_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c5347a-6ff3-4bfe-b76d-bc5f3371cb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_dataset_from_files(data_path, test_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7d82d0a-4fd5-44f6-83a1-2939eae33d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYour task is to generate a conversation between David and Choi. David will ask questions or talk based on his emotions and context, and Choi will respond appropriately according to his role and emotion.<|im_end|>\\n<|im_start|>user\\nPrevious context: David was reminiscing about his early days in Korea, mentioning how he felt a bit lost at first but eventually found his footing.\\nTopic: David’s early days in Korea\\nLanguage: Korean\\nDavid's Emotion: warm\\nChoi's Role: Motivational speaker\\nChoi's Emotion: comforting\\n<|im_end|>\\n<|im_start|>assistant\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = text = tokenizer.apply_chat_template(\n",
    "    dataset['train']['messages'][2],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "test_text = '\\n'.join(test_text.split(\"\\n\")[:-4]) + '\\n'\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1ea78bc-968d-4073-946c-cea6729cac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_test_text = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14cb9de6-72f2-4dea-915b-e89dc41ddac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   7771,   3383,    374,    311,   6923,    264,\n",
       "          10435,   1948,   6798,    323,  86573,     13,   6798,    686,   2548,\n",
       "           4755,    476,   3061,   3118,    389,    806,  21261,    323,   2266,\n",
       "             11,    323,  86573,    686,   5889,  34901,   4092,    311,    806,\n",
       "           3476,    323,  19772,     13, 151645,    198, 151644,    872,    198,\n",
       "          21291,   2266,     25,   6798,    572,  42550,  52754,    911,    806,\n",
       "           4124,   2849,    304,  11862,     11,  44291,   1246,    566,   6476,\n",
       "            264,   2699,   5558,    518,   1156,    714,   9583,   1730,    806,\n",
       "          73403,    624,  26406,     25,   6798,    748,   4124,   2849,    304,\n",
       "          11862,    198,  13806,     25,  16134,    198,  22286,    594,   5748,\n",
       "           5956,     25,   8205,    198,   1143,   6728,    594,  15404,     25,\n",
       "          18977,    344,   1663,  18601,    198,   1143,   6728,    594,   5748,\n",
       "           5956,     25,  68030,    198, 151645,    198, 151644,  77091,    198]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_test_text.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea4b04cd-23af-40a8-a9c1-0076a3ded760",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**tokenizer_test_text, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30fc9425-515a-4c36-a9e8-0208bdd6415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: system\n",
      "Your task is to generate a conversation between David and Choi. David will ask questions or talk based on his emotions and context, and Choi will respond appropriately according to his role and emotion.\n",
      "user\n",
      "Previous context: David was reminiscing about his early days in Korea, mentioning how he felt a bit lost at first but eventually found his footing.\n",
      "Topic: David’s early days in Korea\n",
      "Language: Korean\n",
      "David's Emotion: warm\n",
      "Choi's Role: Motivational speaker\n",
      "Choi's Emotion: comforting\n",
      "\n",
      "assistant\n",
      "David: \"한국에서 처음부터 시작했을 때는 좀 허무하고 막막했어요. 그때부터 점점 적응하고 있더라고요.\"\n",
      "Choi: \"그런 어려움을 겪은 당신이지만, 그것이 결국 당신의 성장을 만들어냈어요. 저는 여러분에게도 그런 경험을 추천하고 싶어요. 당신이 지금 느끼는 허무함과 막막함이 당신의 성장의 단계를 지나치게 빨리 간주하지 말고, 그 흔적들이 당신의 성장을 통해 큰 가치를 가질 수 있다는 것을 기억하세요.\"\n"
     ]
    }
   ],
   "source": [
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\", decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735dd1a2-740c-456e-8a7d-bd998ae50091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
